{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from pickle import load\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data):\n",
    "    data_dict = {\"input_ids\":[]}\n",
    "    for k, v in data.items():\n",
    "        input_ids = tokenizer.encode(v[2])\n",
    "        for i in range(0,len(input_ids),1024):\n",
    "            data_dict[\"input_ids\"].append(input_ids[i:i+512])\n",
    "    return Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(open(\"../Data/SouthPark_Data_train.pkl\", \"rb\"))\n",
    "data = create_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_colator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False,return_tensors='pt')\n",
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2',eos_token_id=tokenizer.eos_token_id,bos_token_id=tokenizer.bos_token_id)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./outputs/',\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_colator,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ai\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2581\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1291\n",
      " 39%|███▊      | 500/1291 [02:52<04:30,  2.92it/s]Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3459, 'learning_rate': 0.0003378192953241824, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./outputs/checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./outputs/checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-500\\special_tokens_map.json\n",
      " 77%|███████▋  | 1000/1291 [05:47<01:38,  2.96it/s]Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.102, 'learning_rate': 6.089972077092024e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./outputs/checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./outputs/checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-1000\\special_tokens_map.json\n",
      "100%|██████████| 1291/1291 [07:32<00:00,  3.31it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1291/1291 [07:32<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 452.0384, 'train_samples_per_second': 5.71, 'train_steps_per_second': 2.856, 'train_loss': 3.166011606418837, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1291, training_loss=3.166011606418837, metrics={'train_runtime': 452.0384, 'train_samples_per_second': 5.71, 'train_steps_per_second': 2.856, 'train_loss': 3.166011606418837, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\Kupus/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to C:\\Users\\Kupus\\.cache\\huggingface\\transformers\\tmpvs0hijv2\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:02<00:00, 551kB/s]\n",
      "storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at C:\\Users\\Kupus/.cache\\huggingface\\transformers\\16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "creating metadata file for C:\\Users\\Kupus/.cache\\huggingface\\transformers\\16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at C:\\Users\\Kupus/.cache\\huggingface\\transformers\\684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at C:\\Users\\Kupus/.cache\\huggingface\\transformers\\c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at C:\\Users\\Kupus/.cache\\huggingface\\transformers\\16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\Kupus/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\"\", max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lot of people in the audience are excited to try and take me back to normal!\n",
      "Chef: Well, let's try to do something with you, Stan.\n",
      "Stan: Dude, hehehehahhhheh! [turns around and goes down the stairs to the ground]\n",
      "Scene Description: The White Houses. Stan and Kyle head to Kyle's house. He runs into the basement, walks around, and finds a stack of paper in the ground. Kyle and the boys step out of it. A voice comes on.\n",
      "Kyle: Hi. Whoa!\n",
      "Whistlin' Voice: Yes, Stanley. [a man is walking on a sofa]\n",
      "Stan: Stanley, come over here. [the man leaves the sofa]\n",
      "Kyle: Jesus! Stan has lost hope in your life.\n",
      "Stan: [wearing a purple hat] Oh! What happened?!\n",
      "Wendy: He had an accident! [Stan and Kyle look at each other] The man who's the most afraid and afraid of the earth! [tears up]\n",
      "Kyle: What the hell is that?!\n",
      "Stan: That's because the father says things are gonna be so bad, and he's gonna drive his son out of town.\n",
      "Kyle: [wearing a purple cap] He was driving the whole idea!\n",
      "Stan: Aaaughh--\n",
      "Kyle: [the man now pulls up behind the sofa and slams it open. Stan looks in and sees all the boys there]\n",
      "Stan: [looking lost at the situation] [wakes up to be interviewed]\n",
      "Scene Description: A house of houses, day. Stan and Kyle sit on the steps, crying. Cartman enters with Cartman to show them.\n",
      "Cartman: Hey, that's so very funny, Kyle.\n",
      "Kyle: [trying to catch his breath] Dude! I don't think I'm seeing all the crap in this house, do it?\n",
      "Cartman: No. [calls out] My house was in a little mess.\n",
      "Kyle: [disappointed-] Dude, who the fuck was that?!\n",
      "Cartman: Oh. Sorry guys, what have all the other boys do? I'm sure this would have gone a pretty bad if we had a picture of you. Who were all those poor little panspermial people in the town of South Park?!\n",
      "Cartman: What?\n",
      "Kyle: Kyle, I think I have all kinds of hiccups right now.\n",
      "Cartman: What are you gonna do after seeing all the crap in the house, huh?! [the boys grin sadly] You boys, Kyle again?!\n",
      "Stan: I don't think that means he's on my side anymore? [they laugh in agreement]\n",
      "Cartman: Whoa who? That's the woman responsible for all the shit that we said?\n",
      "Kyle: I have a new sister! Come on! [they laugh]\n",
      "Kenny: She'll be coming over! [smiles.]\n",
      "Cartman: Why, fatass-ass asshead!\n",
      "Scene Description: Dude: [his father is eating pudding is being kept at the dining. Kenny's house]\n",
      "Cartman: [walks up your son, fatassling. [walks on] Come on, come home, Kenny!\n",
      "Liane: The girl in the bathroom, it was me, dude! [smiles and drops in.]\n",
      "Stan: Hey, come on!\n",
      "Pip: What's goin' on you guys, we have to do for school! [snaps him. Shelly and the pigtails. Kenny keeps a bit.]\n",
      "Randy: Dude!\n",
      "Kyle: So?\n",
      "Kyle: Dude: I have five minutes to come see her work.\n",
      "Scene Description: Kenny's house. He notices the boys, including Kenny, Stan, Kyle and Stan, having to go in a bathroom. Kenny's house and Stan goes down the stairs.\n",
      "Stan: [turns away] Kenny's parents just got us a new friend, the woman?!\n",
      "Stan: Oh. He's pissed!\n",
      "Kyle: Come on, you guys!\n",
      "Cartman: Come on me, it was a night in the cold! [Stan and Kyle walk off]\n",
      "Shelly: Uh, look at that woman.\n",
      "Kyle: All right, listen. Well, what if they wanted to help me the girls out. Shelly's gonna come over. [hands her notes and he's seen Kenny sitting at his own table. \"I want to kill you, please! [they run by the other boys!\" [the others continue to follow.] Don't worry, we'll be playing with Kenny now!!\n",
      "Stuart: You want that's what you're doing with them, Stirling, Cartman! Why are we hanging out with people at the girls, bros?! [the camera pans into place. The adults run on the kids, who don't notice them moving around\n"
     ]
    }
   ],
   "source": [
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da090f050559786aa9031213f726c80bef2475398317cb1ead5e27a23b45e9d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
